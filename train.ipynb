{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'project2_1 (Python 3.8.19)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n project2_1 ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dimensions, action_dimensions, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        # first layer of the neural network, state_dimension amt of neurons to 256 neurons\n",
    "        self.first_layer = nn.Linear(state_dimensions, 256)\n",
    "\n",
    "\n",
    "        self.second_layer = nn.Linear(256, 256)\n",
    "\n",
    "        # 256 neurons to action_dimension amt of neurons\n",
    "        self.third_layer = nn.Linear(256, action_dimensions)\n",
    "\n",
    "        # the scale of the action (-1, 1) for our purposes\n",
    "        self.max_action = max_action\n",
    "\n",
    "\n",
    "    def forward(self, state):\n",
    "        # returns a \"tensor\" (a matrix with a single column) representing the output of the first layer\n",
    "        x = torch.relu(self.first_layer(state))\n",
    "        x = torch.relu(self.second_layer(x))\n",
    "\n",
    "        # returns a tensor with one column and eight rows, each item represents the\n",
    "        # continous action to take on a given control (left axis, right axis, etc)\n",
    "        action = torch.tanh(self.third_layer(x)) * self.max_action\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critic Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dimensions, action_dimensions):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        # a tad different from the neural network setup in the actor, instead of taking in states and outputting actions\n",
    "        # we are taking in a state and an action while outputting a single number (the Q-value)\n",
    "        self.first_layer = nn.Linear(state_dimensions + action_dimensions, 256)\n",
    "        self.second_layer = nn.Linear(256, 256)\n",
    "        self.third_layer = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "\n",
    "        # takes the 8 pieces of the state and the 2 pieces of the action and shapes it into a tensor with 10 neurons\n",
    "        # these neurons altogether tell the network the state and the action taken in that state\n",
    "        x = torch.cat([state, action], 1)\n",
    "\n",
    "        # then we run through the neural network\n",
    "        x = torch.relu(self.first_layer(x))\n",
    "        x = torch.relu(self.second_layer(x))\n",
    "\n",
    "        # eventually ending up with a single value (the Q-value)\n",
    "        value = self.third_layer(x)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the purpose of this replay buffer is to further randomize data\n",
    "# primarily for the purpose of eliminating learning by correlation,\n",
    "# but also it serves as quite the computational speedup, as the \"batches\"\n",
    "# that we run through the critic and the actor get parallelized\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # we gather a batch_size amount of random snapshots from the buffer\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        # and we return them as numpy arrays to be tested on\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 # the amount of snapshots to be run through the critic at once\n",
    "gamma = 0.99  # the discount factor\n",
    "update_rate = 0.001  # the learning rate\n",
    "target_update_rate = 0.0005  # the update rate rate of the target network\n",
    "exploration_noise = 0.1 # the rate at which random actions occur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize environment\n",
    "env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "state_dimensions = env.observation_space.shape[0]\n",
    "action_dimensions = env.action_space.shape[0]\n",
    "max_action = env.action_space.high[0]\n",
    "\n",
    "# initialize networks\n",
    "actor = Actor(state_dimensions, action_dimensions, max_action)\n",
    "critic = Critic(state_dimensions, action_dimensions)\n",
    "target_actor = Actor(state_dimensions, action_dimensions, max_action)\n",
    "target_critic = Critic(state_dimensions, action_dimensions)\n",
    "\n",
    "# copy weights to target networks\n",
    "target_actor.load_state_dict(actor.state_dict())\n",
    "target_critic.load_state_dict(critic.state_dict())\n",
    "\n",
    "# initialize optimizers, to tune the weights of the networks\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=update_rate)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=update_rate)\n",
    "\n",
    "# initialize replay buffer\n",
    "replay_buffer = ReplayBuffer(max_size=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### for graphs and stuff\\nepisode_rewards = []\\ncurrent_Q_means = []\\ntarget_Q_means = []\\nactor_losses = []\\ncritic_losses = []\\n###\\n\\n# we are going to run this many attempts at landing the lunar lander\\nnum_episodes = 1000\\nepisode_reward_dict = {}\\nfor episode in range(num_episodes):\\n\\n    # each of those steps will start the same way\\n    state, _ = env.reset()\\n    episode_reward = 0\\n    done = False\\n\\n    # while the lander hasnt landed\\n    while not done:\\n\\n        # we have the actor select an action\\n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\\n        action = actor(state_tensor).detach().numpy()[0]\\n\\n        # add a bit of randomness to it\\n        action = action + np.random.normal(0, exploration_noise, size=action_dimensions)\\n\\n        # make sure it is still between -1 and 1\\n        action = np.clip(action, -max_action, max_action)\\n\\n        # and take that action\\n        next_state, reward, done, _, info = env.step(action)\\n        episode_reward += reward\\n\\n        # that snapshot (state, action, reward, net_state, done) then gets stored in the buffer\\n        replay_buffer.add(state, action, reward, next_state, done)\\n\\n        # and we enter that state\\n        state = next_state\\n\\n        # once we have a predetermined amount of snapshots\\n        if len(replay_buffer.buffer) > batch_size:\\n\\n            # we random select a predetermined amount of snapshots\\n            states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\\n\\n            # and turn them into tensors, so they can be used as input to the neural networks\\n            states = torch.FloatTensor(states)\\n            actions = torch.FloatTensor(actions)\\n            rewards = torch.FloatTensor(rewards).unsqueeze(1)\\n            next_states = torch.FloatTensor(next_states)\\n            dones = torch.FloatTensor(dones).unsqueeze(1)\\n\\n            # we then see what the target critic deems these [state, action] pairs\\' q-values\\n            with torch.no_grad(): # ensures we dont update the target\\n                next_actions = target_actor(next_states)\\n                target_Q = target_critic(next_states, next_actions)\\n                target_Q = rewards + (1 - dones) * gamma * target_Q\\n                target_Q_means.append(target_Q.mean().item()) # for graphs!\\n\\n            # and we see what the current critic things those q-values are\\n            current_Q = critic(states, actions)\\n            current_Q_means.append(current_Q.mean().item()) # for graphs!\\n\\n            # using the current critic\\'s Q-value and the target critic\\'s Q-value, we caluate the loss\\n            critic_loss = nn.MSELoss()(current_Q, target_Q)\\n            critic_losses.append(critic_loss.item()) # for graphs!\\n\\n            # and we use backpropogation to update the critic\\n            critic_optimizer.zero_grad()\\n            critic_loss.backward()\\n            critic_optimizer.step()\\n\\n            # then we use the critic\\'s feedback on the actor\\'s action to calculate the loss\\n            actor_loss = -critic(states, actor(states)).mean()\\n            actor_losses.append(actor_loss.item()) # for graphs!\\n\\n            # and we use backpropogation to update the actor\\n            actor_optimizer.zero_grad()\\n            actor_loss.backward()\\n            actor_optimizer.step()\\n\\n            # after that, we provide a smaller update to the target actor and target critic\\n            for target_param, param in zip(target_critic.parameters(), critic.parameters()):\\n                target_param.data.copy_(target_update_rate * param.data + (1 - target_update_rate) * target_param.data)\\n            for target_param, param in zip(target_actor.parameters(), actor.parameters()):\\n                target_param.data.copy_(target_update_rate * param.data + (1 - target_update_rate) * target_param.data)\\n\\n    episode_rewards.append(episode_reward) # for graphs!\\n    print(f\"curr episode: {episode}, reward: {episode_reward}\")\\n    episode_reward_dict[episode] = reward\\n\\nenv.close()\\n'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''### for graphs and stuff\n",
    "episode_rewards = []\n",
    "current_Q_means = []\n",
    "target_Q_means = []\n",
    "actor_losses = []\n",
    "critic_losses = []\n",
    "###\n",
    "\n",
    "# we are going to run this many attempts at landing the lunar lander\n",
    "num_episodes = 1000\n",
    "episode_reward_dict = {}\n",
    "for episode in range(num_episodes):\n",
    "\n",
    "    # each of those steps will start the same way\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "\n",
    "    # while the lander hasnt landed\n",
    "    while not done:\n",
    "\n",
    "        # we have the actor select an action\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        action = actor(state_tensor).detach().numpy()[0]\n",
    "\n",
    "        # add a bit of randomness to it\n",
    "        action = action + np.random.normal(0, exploration_noise, size=action_dimensions)\n",
    "\n",
    "        # make sure it is still between -1 and 1\n",
    "        action = np.clip(action, -max_action, max_action)\n",
    "\n",
    "        # and take that action\n",
    "        next_state, reward, done, _, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # that snapshot (state, action, reward, net_state, done) then gets stored in the buffer\n",
    "        replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # and we enter that state\n",
    "        state = next_state\n",
    "\n",
    "        # once we have a predetermined amount of snapshots\n",
    "        if len(replay_buffer.buffer) > batch_size:\n",
    "\n",
    "            # we random select a predetermined amount of snapshots\n",
    "            states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "            # and turn them into tensors, so they can be used as input to the neural networks\n",
    "            states = torch.FloatTensor(states)\n",
    "            actions = torch.FloatTensor(actions)\n",
    "            rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "            next_states = torch.FloatTensor(next_states)\n",
    "            dones = torch.FloatTensor(dones).unsqueeze(1)\n",
    "\n",
    "            # we then see what the target critic deems these [state, action] pairs' q-values\n",
    "            with torch.no_grad(): # ensures we dont update the target\n",
    "                next_actions = target_actor(next_states)\n",
    "                target_Q = target_critic(next_states, next_actions)\n",
    "                target_Q = rewards + (1 - dones) * gamma * target_Q\n",
    "                target_Q_means.append(target_Q.mean().item()) # for graphs!\n",
    "\n",
    "            # and we see what the current critic things those q-values are\n",
    "            current_Q = critic(states, actions)\n",
    "            current_Q_means.append(current_Q.mean().item()) # for graphs!\n",
    "\n",
    "            # using the current critic's Q-value and the target critic's Q-value, we caluate the loss\n",
    "            critic_loss = nn.MSELoss()(current_Q, target_Q)\n",
    "            critic_losses.append(critic_loss.item()) # for graphs!\n",
    "\n",
    "            # and we use backpropogation to update the critic\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "\n",
    "            # then we use the critic's feedback on the actor's action to calculate the loss\n",
    "            actor_loss = -critic(states, actor(states)).mean()\n",
    "            actor_losses.append(actor_loss.item()) # for graphs!\n",
    "\n",
    "            # and we use backpropogation to update the actor\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "\n",
    "            # after that, we provide a smaller update to the target actor and target critic\n",
    "            for target_param, param in zip(target_critic.parameters(), critic.parameters()):\n",
    "                target_param.data.copy_(target_update_rate * param.data + (1 - target_update_rate) * target_param.data)\n",
    "            for target_param, param in zip(target_actor.parameters(), actor.parameters()):\n",
    "                target_param.data.copy_(target_update_rate * param.data + (1 - target_update_rate) * target_param.data)\n",
    "\n",
    "    episode_rewards.append(episode_reward) # for graphs!\n",
    "    print(f\"curr episode: {episode}, reward: {episode_reward}\")\n",
    "    episode_reward_dict[episode] = reward\n",
    "\n",
    "env.close()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pack It Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pickle \\n\\n# save off the model\\ntorch.save(actor.state_dict(), \\'base_run4.pth\\')\\n\\n# pack up the graph data\\ndata_to_save = {\\n    \"episode_rewards\": episode_rewards,\\n    \"current_Q_means\": current_Q_means,\\n    \"target_Q_means\": target_Q_means,\\n    \"actor_losses\": actor_losses,\\n    \"critic_losses\": critic_losses\\n}\\n\\n# save off the graph data\\nwith open(\\'base_run4.pkl\\', \\'wb\\') as f:\\n    pickle.dump(data_to_save, f)'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pickle \n",
    "\n",
    "# save off the model\n",
    "torch.save(actor.state_dict(), 'base_run4.pth')\n",
    "\n",
    "# pack up the graph data\n",
    "data_to_save = {\n",
    "    \"episode_rewards\": episode_rewards,\n",
    "    \"current_Q_means\": current_Q_means,\n",
    "    \"target_Q_means\": target_Q_means,\n",
    "    \"actor_losses\": actor_losses,\n",
    "    \"critic_losses\": critic_losses\n",
    "}\n",
    "\n",
    "# save off the graph data\n",
    "with open('base_run4.pkl', 'wb') as f:\n",
    "    pickle.dump(data_to_save, f)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project2_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
